
<!-- 在 markdown 顶部加入 MathJax 支持 -->
<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

前一章中我们介绍了机器学习的主要领域、根据目标的分类，以及简化版的一元线性回归。但在实际运用中，我们需要处理的数据常常有多个特征。此时我们就需要将一元线性回归拓展到更高的维度，以拟合更多的特征。

# 多元线性回归,损失函数与线性回归的矩阵求解

在上一章中，我们以房价预测为例介绍了一元线性回归，并假设价格仅与面积线性相关。然而现实中，影响价格的因素远不止面积，比如卧室数量、楼层、房龄等都可能是决定价格的重要特征。

为了应对这些多维因素，我们需要将一元线性回归扩展为多元线性回归模型。


![factors](./factors.jpg)

由于引入了新的特征，我们的符号表示也会相应地复杂起来，现在让我们明确:

$x_j$ :第
$j$ 个特征，
$j=1，2，3，....$

$n$ :特征的数量，在上述表中，
$n=4$

$\vec{x}^{(i)}$:
第 $i$ 组训练数据

$x^{(i)}_j$:
第 $i$ 组训练数据中
第 $j$ 个特征的值，
如在上表中 $x^{(2)}_3=2$

![factors——includ](./factors——includ.jpg)

相应地，我们的模型就从一元线性

$$
f_{w,b}(x)=wx+b
$$

变为：

$$
f_{w,b}(x)=w_1x_1+w_2x_2+w_3x_3+w_4x_4+b
$$

也就是说，我们使用了新的 **多元线性回归模型** :

$$
f_{w,b}(x)=w_1x_1+w_2x_2+...+w_nx_n+b
$$

相应地，梯度下降更新为：

$$
\begin{aligned}
repeat &:\\
w_j&:=w_j-\frac{\partial}{\partial w_j}J(\vec{w},b)\\
b&:=b-\frac{\partial}{\partial b}J(\vec{w},b)
\end{aligned}
$$

其中：

$\nabla_{\vec{w}} J(\vec{w},b)=\frac{1}{m}\sum^{i=1}_{m}\left( f_{\vec{w},b}(x^{(i)})-y^{(i)}\right)x^{(i)}$

$\nabla_b J(\vec{w},b)=\frac{1}{m}\sum^{i=1}_{m}\left( f_{\vec{w},b}(x^{(i)})-y^{(i)}\right)$

## 损失函数的性质

随着我们进展的深入，以及模型的复杂化，我们必须简单解释一下我们选择

$$

J=\frac{1}{2m} \sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})^2

$$

作为损失函数的原因

首先，出于最朴素的原因，我们想衡量我们的预测值 $f(x^{(i)})$ 与真实样本
$y^{(i)}$ 之间的差，我们希望衡量预测值和真实值的差异，最直观的方式是“取差值”。但就像一个学生考试错一题加 5 分、对一题减 5 分，总分可能看起来是零，这掩盖了真实的波动。

为避免正负误差相互抵消，我们使用平方将其“全部拉为正数”，从而真实反映总误差的大小。

$f(x^{(i)})-y^{(i)}$

但简单地对它们求差，可能得到一些正负不一的差值，他们会相互抵消，因此我们对他们求一个平方，即：

$(f(x^{(i)})-y^{(i)})^2$

**为什么是求平方，而不是绝对值等其他处理方法？**

以下解释在我的文章结构里可能导致一些先射箭再画靶的观感，但是我希望简单地解释一下在最小二乘损失在线性回归中所有的良好性质

**在线性回归和一些其他模型中。我们默认假设误差项服从正态分布**，这其实是很多机器学习/统计模型在建模背后的重要建模假设，但常常没有被充分解释，就像给你一个洛必达公式就让你去解高考最后一大题导数。受迫于篇幅有限，我希望在我简单提到后大家可以有兴趣查找相关资料，完善自己的知识储备。

### 什么是“误差服从正态分布”？

在回归模型中，我们常将样本观测值，模型预测值和误差值(噪声)之间的关系写为：

$$
y_i=f(x_i)+\epsilon_i
$$

在这背后，我们假设 $\epsilon_i～\mathcal{N}(0,\sigma^2)$
也就是说，我们假设误差值是一个 **均值为0** (无系统偏移)，**方差固定** (同方差性), **独立同分布** 的 **正态分布** (或高斯分布),支持这一假设最核心的理由是 **中心极限定理** ，即：

$$
\root\of{N}\left(\frac{1}{N}\sum^N_{i=1}(X_i-\mathbb{E}[x_1])\right)\to_d\mathcal{N}(0,\sigma^2)
$$

**直观地说，如果一个变量是多个独立随机因素的综合，它的分布将趋近于正态分布**

当误差来自于多而小的独立随机扰动，特征间可近似看作独立时，总误差可以看作正态分布，此时对模型采用正态分布假设是合适的。

若存在显著离群点(Outliers)，正态分布对极端值非常敏感，此时应考虑更为鲁棒(Robust)的分布，如拉普拉斯分布(Laplace Distribution)、Huber损失等

当误差有偏态(Skewek)或为长尾分布(Heavy-tailed Distribution)时，则应该考虑使用Student-t检验或泊松分布(Poisson Distribution)等。

正态分布假设所有样本的误差方差都相同，但现实中我们常常会遇到不同输入区域下噪声强度不同等情况。此时我们则必须考虑将模型换为加权最小二乘(WLS)或广义线性模型(GLM)进行替代。

总之，在假设的选取中，我们需要格外注意，选取一个符合已有数据分布的模型。

### 正态分布的性质

正态分布拥有许多良好的性质，例如正态分布具有解析形式，且可导、光滑，这意味着在进行求梯度等操作时方便处理。在正态分布下，根据Guass-Markov定理:在线性模型中，若误差为零均值、独立、同方差，则最小二乘估计时最佳线性无偏估计(BLUE)。

### 极大似然估计(Maximum Likelihood Estimation)视角下的损失函数推导


**观前提醒：以下会涉及一些令人不适的数学推导，不想看可以跳过**

极大似然估计就像你在看到骰子连续5次掷出6点后，不禁怀疑这个骰子有猫腻。你会试图猜测出哪个模型最可能生成这组奇怪的结果。你可以提出一些猜测(比如在他的骰子中某些点更容易出现)，但你最终会选择那个 **最有可能导致出现这个数据的模型参数**。

极大似然估计做的就是这件事:它假设某个模型生成了数据，然后 **反过来推测最可能导致出现这个数据的模型参数**

也就是：

假设我们摇出了骰子 $[6,6,6,6,6]$

形式化一些写为： $[x_1,x_2,x_3,x_4,x_5]$

假设每个骰子的结果都是独立同分布的，则得到特定结果的联合概率为： 
$$
\mathcal{L}(\theta)=\prod_{i=1}^n \mathbb{p}(x_i \mid \theta)
$$

此时我们将针对固定的 $[x_1,x_2,x_3,x_4,x_5]$ ，也就是将
$L(\theta)$ 看作对 $\theta$ 的函数

极大似然估计所做的是——通过寻找使事件发生概率最大的参数，从而得到最合理地解释现象的一组参数

也就是使：

$$
\mathbb{p}(x_i|\theta)=\mathbb{P}(x_1,x_2,...,x_n|\theta)
$$

最大时模型的参数 $\theta$

对于给定观测数据 $D=\{x_1,x_2,...,x_n\}$
和假设的数据分布 $p(x|\theta)$ ，

极大似然估计的目标是：

$$
\hat{\theta} = \arg\max_\theta \prod_{i=1}^n \mathbb{p}(x_i \mid \theta)
$$

也就是最大化当前数据在该模型下的联合概率

#### 以误差服从正态分布的线性回归为例：

线性回归中我们常设模型为 $y=wx+b+\epsilon$

误差服从正态分布 $\epsilon～\mathcal{N}(0,\sigma^2)$

数据集为 $D={x_i,y_i}$

对于正态分布，其概率密度为：

$$
f(x)=\frac{1}{\sigma\root\of{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

则对于每个 $x_i$ ,概率密度为：

$$
\mathbb{p}(y \mid x_i; w, b) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(y_i-wx_i-b)^2}{2\sigma^2} \right)
$$

其中 $(y_i-wx_y-b)$ 衡量了预测值于真实值之间的残差

假设样本间独立同分布，则联合似然函数为：

$$
\mathcal{L}(w, b) = \prod_{i=1}^n \mathbb{p}(y_i \mid x_i; w, b)
$$

代入密度函数，则：

$$
\mathcal{L}(w, b) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{(y_i - wx_i - b)^2}{2\sigma^2} \right)
$$

由于乘积形式不便于处理，因此我们通常取对数处理：

$$
\log\mathcal{L}(w,b)=\sum^n_{i=1}\log\left[\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(y_i-wx_i-b)^2}{2\sigma^2} \right)\right]
$$

我们对上式一步一步拆解：

$\log\mathcal{L}(w,b)=\sum^n_{i=1} \left[ \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) + \log \left( \exp\left( - \frac{(y_i - wx_i - b)^2}{2\sigma^2} \right) \right) \right]$

利用对数的性质可知：

$\log\mathcal{L}(w,b)=\sum_{i=1}^n \left[ -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(y_i - wx_i - b)^2}{2\sigma^2} \right]$

对前后两项分开求和：

$\log \mathcal{L}(w, b) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - wx_i - b)^2$

由于前一项

$-\frac{n}{2}\log(2\pi\sigma^2)$ 与 $w,b$ 无关，

因此最大化对数似然 $maxlog\mathcal{L}(w,b)$ 就等价于最小化第二项 

$- \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - wx_i - b)^2$

即最小化：

$$
- \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - wx_i - b)^2
$$

也就等价于最小化平方误差：

$$
\min J(w, b) = \sum_{i=1}^n (y_i - wx_i - b)^2
$$

**因此，当我们假设误差项服从正态分布时，用极大似然估计(Maximize Likelihood Estimate)推导出的最优模型参数等价于最小化平方误差(最小二乘损失)**

需要注意的是，正态分布只是模型的一个“假设”，而不是对现实的描述。如果数据的误差偏离正态分布，例如存在离群点或重尾分布，那么基于最小二乘的回归可能性能较差。

### 那么如果我们的数据误差不服从正态分布呢？

假设我们现有的数据误差服从拉普拉斯分布（Laplace Distribution），它是一种 **比正态分布更具鲁棒性** 的分布，虽然**不是严格意义上的重尾分布**（如 Cauchy 或 Student-t 分布），但其对异常值（outliers）相对更敏感，适合建模含有离群点的数据。

需要注意的是，**“重尾分布（heavy-tailed）”与“长尾分布（long-tail）”不是同一个概念**：

- **重尾分布** 是统计学中对尾部衰减速度的正式定义，指其尾部概率比指数衰减慢，例如：Pareto 分布、Cauchy 分布、t 分布等。
- **长尾分布** 更多是互联网与商业场景的非正式用语，强调大量低频事件的存在，常见于电商推荐、内容分发等应用。

拉普拉斯分布的尾部介于正态与真正的重尾分布之间，因此在某些文献中被称为“次重尾”（sub-exponential）分布。


拉普拉斯分布定义为：

$$
\mathbb{p}(\varepsilon) = \frac{1}{2\beta} \exp\left( -\frac{|\varepsilon|}{\beta} \right)
$$

在线性模型 $y_i=wx_i+b+\epsilon_i$ 中， $\epsilon_i=y_i-wx_i-b～Laplace(0,\beta)$

因此可得每个样本点的概率密度为：

$$
p(y_i|x_i) = \frac{1}{2\beta} \exp\left( -\frac{|y_i - wx_i - b|}{\beta} \right)
$$

同样地我们取对数：

$$
\log \mathcal{L}(w, b) = -\frac{1}{\beta} \sum_{i=1}^n |y_i - wx_i - b|
$$

极大似然等价于最小化绝对误差损失，即：

$$
\mathcal{L}_{\text{laplace}}(w, b) = \sum_{i=1}^n |y_i - wx_i - b|
$$

这里得到的即为绝对误差损失，也称为L1损失，区别于此前的最小二乘损失，也称L2损失

### 最小二乘损失的凸性(Convex)

最小二乘损失的另一个良好性质就是其凸性。凸性是函数的一种几何特性，作为一个凸函数，其所有的“局部最小值”就是“全局最小值”，换言之，其“局部最低点”一定就是“全局最优点”，这在机器学习模型的优化中十分重要。

对于任意的 $x_1,x_2\in\mathbb{R}$ , 任意 $\lambda\in[0,1]$ , 函数 $f(x)$ 是凸的,当且仅当：

$$
f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
$$

![convex chord](./img/convex_chord.png)

通俗来说：以函数图像上任意两点作一条直线，如果这条直线永远在图像的上方，则这个函数是凸函数

若 $f(x)$ 是二阶可导函数，则：

若 $ f^{''}(x)\geq 0 $ 则 $f(x)$ 是凸函数

若 $ f^{''}(x)\gt 0 $ 则 $f(x)$ 是严格凸函数(只有一个最小值)


对于非凸函数，模型很有可能下降并受限于一个局部的最小值，而不是全局的最优解。而对于凸函数，以同一组数据由不同的初始点开始进行的优化，最终都会收敛到同一个最优点附近，这使得模型训练的结果具有可重复性。

另外，许多凸损失函数甚至可以通过公式计算出解析解，而不需要经过复杂的迭代优化，计算开销小。


## 线性回归的矩阵求解

我们假设误差项服从正态分布，那么极大似然估计下的最优参数就转化为最小化平方损失的问题。平方损失是凸函数，具有解析解，且解为唯一最小值。因此我们可以使用线性代数的方法，“优雅”地写出其闭式解

### 向量化(Vectorization)

我们将特征和权重 **向量化(Vectorization)** 

假设训练数据集包含 $n$ 个样本,每个样本 $x_i \in \mathbb{R}^d$ , 标签为 $y_i \in \mathbb{R}$

对第 $i$ 组样本，特征向量为：

$$
\vec{x^{(i)}}=[x^{(i)}_1,x^{(i)}_2,x^{(i)}_3,...x^{(i)}_d]\in \mathbb{R}^d
$$

权重向量为：

$$
w = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_d \end{bmatrix} \in \mathbb{R}^{d}
$$


则我们可以将特征的集合写为特征矩阵 $X$ :

$$
X = \begin{bmatrix}
x^1 \\
x^2 \\
\vdots \\
x^n
\end{bmatrix}
\in \mathbb{R}^{n \times d}
$$

标签向量 $Y$ 为：

$$
Y = \begin{bmatrix}
y^1 \\
y^2 \\
\vdots \\
y^n
\end{bmatrix}
\in \mathbb{R}^{n}
$$

则向量化后的模型预测表达式为：

$$
\hat y=Xw+b
$$

改写损失函数为：

$$
J(w, b) = \| y - Xw - b\mathbf{1} \|_2^2
$$

### 线性代数求解

接下来使用线性代数方法直接对矩阵进行求解

**大量矩阵运算警告！(只关注结论可以跳过)**

对L2范数进行展开：

$$
J(w, b) = (y - Xw - b\mathbf{1})^\top (y - Xw - b\mathbf{1})
$$

继续展开：

$$
\begin{aligned}
J(w, b) 
&= y^\top y - 2y^\top Xw - 2b y^\top \mathbf{1} \\
&\quad + w^\top X^\top X w + 2b w^\top X^\top \mathbf{1} + b^2 \mathbf{1}^\top \mathbf{1}
\end{aligned}
$$

对 $w$ 和 $b$ 联立最小值条件，我们构造使 $J(w,b)$ 最小时满足的线性方程组

联合二次型：

$$
\begin{aligned}
J(w, b) = 
\begin{bmatrix}
w^\top & b
\end{bmatrix}
\begin{bmatrix}
X^\top X & X^\top \mathbf{1} \\
\mathbf{1}^\top X & \mathbf{1}^\top \mathbf{1}
\end{bmatrix}
\begin{bmatrix}
w \\ b
\end{bmatrix}
-
2
\begin{bmatrix}
y^\top X & \mathbf{1}^\top y
\end{bmatrix}
\begin{bmatrix}
w \\ b
\end{bmatrix}
+ \text{C}
\end{aligned}
$$

根据二次型最小值的标准结构

$$
M
\begin{bmatrix}
w\\
b\\
\end{bmatrix}
=v
$$

即解线性方程组：

$$
\begin{bmatrix}
X^\top X & X^\top \mathbf{1} \\
\mathbf{1}^\top X & n
\end{bmatrix}
\begin{bmatrix}
w \\ b
\end{bmatrix}
=
\begin{bmatrix}
X^\top y \\
\mathbf{1}^\top y
\end{bmatrix}
$$

通过普通线性方程组求解方法(如高斯消元、矩阵求逆等)解这个 $(d+1)\times (d+1)$ 的线性系统可以得到：

$$
\begin{bmatrix}
w^\ast \\ b^\ast
\end{bmatrix}
=
\left(
\begin{bmatrix}
X^\top X & X^\top \mathbf{1} \\
\mathbf{1}^\top X & n
\end{bmatrix}
\right)^{-1}
\begin{bmatrix}
X^\top y \\
\mathbf{1}^\top y
\end{bmatrix}

$$

- Note：对于矩阵 
$$
A=\left(
\begin{bmatrix}
X^\top X & X^\top \mathbf{1} \\
\mathbf{1}^\top X & n
\end{bmatrix}
\right)^{-1}
$$
- 要使该矩阵可逆，则 $A$ 必须为非奇异矩阵，即满足 $\det(A)\neq 0$


由此我们可以利用最小二乘损失的优良性质，使用矩阵运算直接进行求解，而避开了大量复杂的多元梯度下降。但是这种简单直接的求解方法只在一部分参数量小、结构简单的模型中起作用。在实际应用中，面对维度极高、结构复杂的数据，我们通常无法依赖解析解，这时就需要借助数值优化方法，诸如梯度下降等高效算法。
